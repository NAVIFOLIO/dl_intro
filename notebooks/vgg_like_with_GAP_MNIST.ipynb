{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Average Pooling(GAP) Layer with simple CNN\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NAVIFOLIO/dl_intro/blob/main/notebooks/vgg_like_with_GAP_MNIST.ipynb)\n",
    "\n",
    "Try [Global Average Pooling layer(Min Lin, et al, 2014)](https://arxiv.org/abs/1312.4400) in simple CNN.\n",
    "\n",
    "Global Average Pooling レイヤーを試そう\n",
    "\n",
    "解説記事（日本語）：[Global Average Pooling(GAP)をCNNで使う](https://navifolio-jp.com/cnn-global-average-pooing)\n",
    "\n",
    "## MNIST Dataset Loading and Pytorch Dataset creation\n",
    "\n",
    "MNISTデータセットを読み込み、PytorchのDatasetを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "mnist_train = MNIST(root=\"./input\", train=True, download=True, transform=transform_train,)\n",
    "mnist_test = MNIST(root=\"./input\", train=False, download=True, transform=transform_test,)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data, lagels = iter(train_loader).__next__()\n",
    "print(f'number of train data: {len(mnist_train)}, number of test data: {len(mnist_test)}')\n",
    "print(f'train data shape: {data.size()}, test data shape: {data.size()}')\n",
    "\n",
    "n_view_image = 25\n",
    "view_loader = DataLoader(dataset=mnist_train, batch_size=n_view_image, shuffle=True)\n",
    "classes = np.array([\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"])\n",
    "view_dataiter = iter(view_loader)\n",
    "images, view_labels = view_dataiter.__next__()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(n_view_image):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(np.transpose(images[i], (1, 2, 0)))\n",
    "    label = classes[view_labels[i]]\n",
    "    plt.title(label)\n",
    "    plt.tick_params(labelbottom=False, labelleft=False, bottom=False, left=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN model like VGG with GAP layer | GAPレイヤーを使用したモデルの作成\n",
    "\n",
    "I created VGG-like but very small CNN, and replace Max Pooling layer just before classifier to Global Average Pooling Layer.\n",
    "Unlike naive VGG, CNN model below include batch normalization layer due to training stability. \n",
    "\n",
    "[VGG](https://arxiv.org/abs/1409.1556)を参考に、非常に小さなCNNモデルを作成します。分類器の前に使用される最大値Poolingの代わりに、Global Average Pooling　Layerを採用します。\n",
    "ただし、素朴なVGGとは異なり、下記のモデルでは学習の安定化のためにバッチ正規化を行なっています。\n",
    "\n",
    "reference: [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class mini_vgg(nn.Module):\n",
    "    def __init__(self, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(256, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, 10, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.norm3 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.globalAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        if init_weights:\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.norm1(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.norm2(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.norm3(self.conv6(x)))\n",
    "        x = self.globalAvgPool(x)\n",
    "        \n",
    "        x = x.view(-1, 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "net = mini_vgg()\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and record loss and accuracy\n",
    "\n",
    "モデルの訓練と損失・正解率の記録を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "record_loss_train = []\n",
    "record_loss_test = []\n",
    "record_accuracy_train = []\n",
    "record_accuracy_test = []\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    net.train()\n",
    "    loss_train = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    print(f'Training of Epoch:{epoch} Start.')\n",
    "    for j, data in enumerate(train_loader):\n",
    "        x, labels = data\n",
    "        x, labels = x.cuda(), labels.cuda()\n",
    "        y = net(x)\n",
    "        loss = loss_func(y, labels)\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "        correct_train += (y.argmax(1) == labels).sum().item()\n",
    "        total_train += len(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_train /= (j + 1)\n",
    "    record_loss_train.append(loss_train)\n",
    "    accuracy_train = correct_train / total_train\n",
    "    record_accuracy_train.append(accuracy_train)\n",
    "    \n",
    "    net.eval()\n",
    "    loss_test = 0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    print(f'Evaluation of Epoch:{epoch} Start.')\n",
    "    for j, data in enumerate(test_loader):\n",
    "        x_test, labels_test = data\n",
    "        x_test, labels_test = x_test.cuda(), labels_test.cuda()\n",
    "        y_test = net(x_test)\n",
    "        loss = loss_func(y_test, labels_test)\n",
    "        loss_test += loss.item()\n",
    "        correct_test += (y_test.argmax(1) == labels_test).sum().item()\n",
    "        total_test += len(x_test)\n",
    "    \n",
    "    loss_test /= (j + 1)\n",
    "    record_loss_test.append(loss_test)\n",
    "    accuracy_test = correct_test / total_test\n",
    "    record_accuracy_test.append(accuracy_test)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss_Train {loss_train}, Loss_Test: {loss_test}\")\n",
    "    print(f\"Epoch: {epoch}, Acc_Train {accuracy_train*100}, Acc_Test: {accuracy_test*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing loss and accuracy by pyplot\n",
    "\n",
    "`matplotlib.pyplot`ライブラリで、エポックごとの損失と精度（正解率）の推移をグラフ化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n",
    "plt.plot(range(len(record_loss_test)), record_loss_test, label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(record_accuracy_train)), record_accuracy_train, label=\"Train\")\n",
    "plt.plot(range(len(record_accuracy_test)), record_accuracy_test, label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating percentage of correct answers for training data.\n",
    "\n",
    "訓練済みモデルを使用し、訓練データに対しての正解率を百分率で計算し表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.eval()\n",
    "\n",
    "for i, (x, t) in enumerate(test_loader):\n",
    "    x, t = x.cuda(), t.cuda()\n",
    "    y = net(x)\n",
    "    correct += (y.argmax(1) == t).sum().item()\n",
    "    total += len(x)\n",
    "\n",
    "print(\"Accuracy:\", str(correct/total*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with no GAP\n",
    "\n",
    "GAPを使用しないモデルとの比較を行いたい方のために、以下にGAPを使用しないVGGベースのモデルを掲載します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class mini_vgg_with_no_gap(nn.Module):\n",
    "    def __init__(self, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(4096, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, 10, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.norm3 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.avgPool = nn.AdaptiveAvgPool2d(4)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        if init_weights:\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.norm1(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.norm2(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.norm3(self.conv6(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.avgPool(x)\n",
    "        \n",
    "        x = x.view(-1, 4096)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
