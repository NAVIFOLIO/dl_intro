{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Google Tensor Processing Unit to train your model which need big memory\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NAVIFOLIO/dl_intro/blob/main/notebooks/trial_TPU_cifar10.ipynb)\n",
    "\n",
    "Try google TPU as acceelerator to train your model.\n",
    "Code below is a example of implementation which use single TPU unit for training mini-CNN.\n",
    "Google Colab TPU provides big memory in free at the moment (Jan, 2025).\n",
    "\n",
    "Google社のTPUを使って、モデルをトレーニングしてみよう。1つのTPUユニットを使用して小さなCNNを訓練してみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install torch_xla library to your colab environment\n",
    "\n",
    "Install torch_xla library and make sure your pytorch version is compatible with torch_xla.\n",
    "\n",
    "PyTorch経由でTPUを利用するためには、torch_xlaライブラリが必要です。\n",
    "下記のコマンドをColab上で実行し、torch_xla ライブラリを導入してください。\n",
    "また、pytorch_xlaライブラリと互換性のあるPyTorchのバージョンをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch~=2.5.0 torch_xla[tpu]~=2.5.0 -f https://storage.googleapis.com/libtpu-releases/index.html -f https://storage.googleapis.com/libtpu-wheels/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CIFAR-10 and create Pytorch Dataset\n",
    "\n",
    "Load [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) in `torchvision.datasets` and use it as `DataLoader`.\n",
    "\n",
    "CIFAR-10データセットをロードし、ミニバッチ学習の準備を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomAffine([-10, 10], scale=(0.8, 1.2)),\n",
    "    transforms.RandomHorizontalFlip(p = 0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0)), \n",
    "])\n",
    "\n",
    "cifar10_train = CIFAR10(root=\"./input\", train=True, download=True, transform=transform_train)\n",
    "cifar10_test = CIFAR10(root=\"./input\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "batch_size = 500 \n",
    "train_loader = DataLoader(dataset=cifar10_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=cifar10_test, batch_size=len(cifar10_test), shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your model\n",
    "\n",
    "モデルの作成を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class mini_vgg(nn.Module):\n",
    "    def __init__(self, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(256, 256, bias=True)\n",
    "        self.fc2 = nn.Linear(256, 10, bias=True)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.norm2 = nn.BatchNorm2d(128)\n",
    "        self.norm3 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.globalAvgPool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        if init_weights:\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, nn.Conv2d):\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.norm1(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.norm2(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.norm3(self.conv6(x)))\n",
    "        x = self.globalAvgPool(x)\n",
    "        \n",
    "        x = x.view(-1, 256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "model = mini_vgg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training your model on TPU device\n",
    "\n",
    "1. Transfer your tensor and model to TPU memory.\n",
    "2. Execute `xm.mark_step()` per training to tell XLA to memorize learning state.\n",
    "\n",
    "モデルの訓練を行います。\n",
    "1. TensorをTPUのメモリに転送する\n",
    "2. 学習毎に、`xm_mark_step()`で学習の状態をXLAデバイスに記録させる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "record_loss_train = []\n",
    "record_loss_test = []\n",
    "\n",
    "def training():\n",
    "  device = xm.xla_device()\n",
    "\n",
    "  loss_func = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "  x_test, t_test = iter(test_loader).__next__()\n",
    "  x_test, t_test = x_test.to(device), t_test.to(device)\n",
    "\n",
    "  for epoch in range(15):\n",
    "      model = model.train().to(device)\n",
    "      loss_train = 0\n",
    "      for j, (data, target) in enumerate(train_loader):\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "          y = model(data)\n",
    "          loss = loss_func(y, target)\n",
    "          loss_train += loss\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          xm.mark_step()\n",
    "\n",
    "      loss_train /= (j + 1)\n",
    "      record_loss_train.append(loss_train)\n",
    "\n",
    "      model = model.eval().to(device)\n",
    "      y_test = model(x_test)\n",
    "      loss_test = loss_func(y_test, t_test).item()\n",
    "      record_loss_test.append(loss_test)\n",
    "    \n",
    "      print(f\"Epoch: {epoch}, Loss_Train: {loss_train}, Loss_Test: {loss_test}\")\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing loss by pyplot\n",
    "\n",
    "Model in this notebook, `mini_vgg`, cannot achieve very high accuracy to\n",
    "CIFAR-10. But you can test training which needs big memory, due to relatively big data size and training batch size.\n",
    "\n",
    "Note: Fetch tensor you transfer to TPU to CPU.\n",
    "\n",
    "matplotlib.pyplotライブラリで、エポックごとの損失と精度（正解率）の推移をグラフ化します。\n",
    "モデルが小さすぎる（層が浅すぎる）ためおそらく高い精度になりませんが、大容量メモリを必要とする学習を（クラッシュせずに）実行するテストを行うことができます。\n",
    "\n",
    "TPUに転送されたTensorをCPUにフェッチする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fetch tensor from TPU memory\n",
    "plt.plot(range(len(record_loss_train.cpu())), record_loss_train.cpu(), label=\"Train\")\n",
    "plt.plot(range(len(record_loss_test.cpu())), record_loss_test.cpu(), label=\"Test\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating percentage of correct answers for training data.\n",
    "\n",
    "Note: Fetch tensor you transfer to TPU to CPU.\n",
    "\n",
    "正解率の算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "device  = xm.xla_device()\n",
    "for i, (x, t) in enumerate(test_loader):\n",
    "    x = x.to(device)\n",
    "    y = model(x)\n",
    "    # Fetch tensor from TPU memory\n",
    "    z = y.cpu()\n",
    "    correct += (z.argmax(1) == t).sum().item()\n",
    "    total += len(x)\n",
    "print(\"Accuracy[%]: \", str(correct/total*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [The CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- [PyTorch/XLA documentation](https://pytorch.org/xla/release/r2.5/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
